{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Self, Optional\n",
    "from collections import namedtuple, deque\n",
    "from dataclasses import dataclass\n",
    "from itertools import count\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Distribution, Normal\n",
    "from torch.optim import Optimizer, Adam\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snapshot shows the pseudocode of the PPO-Clip algorithm:\n",
    "\n",
    "<img src=\"../figures/ppo-clip.png\" width=500px></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space dim: 3\n",
      "action space dim: 1\n"
     ]
    }
   ],
   "source": [
    "def get_state_dim(env: Env) -> Optional[int]:\n",
    "    \n",
    "    try:\n",
    "        return env.observation_space.shape[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_action_dim(env: Env) -> Optional[int]:\n",
    "    \n",
    "    try:\n",
    "        return env.action_space.shape[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "print(f\"state space dim: {get_state_dim(env)}\")\n",
    "print(f\"action space dim: {get_action_dim(env)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim: 3\n",
      "action spec: ActionSpec(dim=1, min=array([-2.], dtype=float32), max=array([2.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ActionSpec:\n",
    "    dim: int\n",
    "    min: np.ndarray\n",
    "    max: np.ndarray\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls, env: Env) -> Self:\n",
    "        return cls(\n",
    "            dim=env.action_space.shape[0],\n",
    "            min=env.action_space.__dict__[\"low\"],\n",
    "            max=env.action_space.__dict__[\"high\"]\n",
    "        )\n",
    "\n",
    "state_dim = get_state_dim(env)\n",
    "action_spec = ActionSpec.from_env(env)\n",
    "\n",
    "print(f\"state dim: {state_dim}\")\n",
    "print(f\"action spec: {action_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and Critic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            out_dim: int\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "    \n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        \n",
    "        x = self.layer1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: tensor([0.4161])\n",
      "log-probability: -0.7357034683227539\n"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim: int,\n",
    "            action_spec: ActionSpec\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self._action_min = torch.tensor(action_spec.min, dtype=torch.float)\n",
    "        self._action_max = torch.tensor(action_spec.max, dtype=torch.float)\n",
    "        \n",
    "        self._distribution = None\n",
    "        self._action = None\n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            in_dim=state_dim,\n",
    "            out_dim=action_spec.dim\n",
    "        )\n",
    "        \n",
    "        self.dist_param1 = nn.Sequential(\n",
    "            nn.Linear(action_spec.dim, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, action_spec.dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dist_param2 = nn.Sequential(\n",
    "            nn.Linear(action_spec.dim, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, action_spec.dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def distribution(self) -> Optional[Distribution]:\n",
    "        \n",
    "        return self._distribution\n",
    "        \n",
    "    \n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        \n",
    "        out: Tensor = self.mlp(state)\n",
    "    \n",
    "        param1 = self.dist_param1(out)\n",
    "        param2 = self.dist_param2(out)\n",
    "        \n",
    "        mean = F.sigmoid(out) * (self._action_max - self._action_min) + self._action_min\n",
    "        # mean = param1 * (self._action_max - self._action_min) + self._action_min\n",
    "        # std = param2\n",
    "        std = torch.ones_like(mean) * 0.5\n",
    "        \n",
    "        # mean = F.sigmoid(out[..., 0]) * (self._action_max - self._action_min) + self._action_min\n",
    "        # std = torch.ones_like(mean) * 0.5\n",
    "        \n",
    "        # Generate the distribution\n",
    "        distribution = Normal(mean, std)\n",
    "        \n",
    "        # Store the distribution\n",
    "        self._distribution = distribution\n",
    "        \n",
    "        return distribution\n",
    "    \n",
    "    def select_action(\n",
    "            self, \n",
    "            state: Optional[Tensor] = None\n",
    "        ) -> Tensor:\n",
    "        \n",
    "        if state is not None:\n",
    "            self.forward(state)\n",
    "\n",
    "        # Sample an action from the distribution\n",
    "        action = self._distribution.sample()\n",
    "        \n",
    "        # Clip the action value by upper and lower bounds\n",
    "        action = action.clip(self._action_min, self._action_max)\n",
    "        \n",
    "        # Store the action taken\n",
    "        self._action = action\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def log_prob(\n",
    "            self, \n",
    "            action: Optional[Tensor] = None,\n",
    "            state: Optional[Tensor] = None\n",
    "        ) -> Tensor:\n",
    "        \n",
    "        if action is None:\n",
    "            assert state is None,\\\n",
    "                \"the state must be set None since the action is None\"\n",
    "            return self._distribution.log_prob(self._action).sum(dim=-1)\n",
    "        \n",
    "        if state is not None:\n",
    "            self.forward(state)\n",
    "            \n",
    "        return self._distribution.log_prob(action).sum(dim=-1)\n",
    "\n",
    "actor = Actor(state_dim, action_spec)\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float)\n",
    "action = actor.select_action(state)\n",
    "log_prob = actor.log_prob()\n",
    "\n",
    "print(f\"action: {action}\")\n",
    "print(f\"log-probability: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: tensor([-0.0888], grad_fn=<AddBackward0>), scale: tensor([0.5000]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated state value: 0.1734076738357544\n"
     ]
    }
   ],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim: int) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.mlp = MLP(\n",
    "            in_dim=state_dim,\n",
    "            out_dim=1\n",
    "        )\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        \n",
    "        return self.evaluate(state)\n",
    "    \n",
    "    def evaluate(self, state: Tensor) -> Tensor:\n",
    "        \n",
    "        out: Tensor = self.mlp(state)\n",
    "        value = out.squeeze(dim=-1)\n",
    "        \n",
    "        return value\n",
    "\n",
    "critic = Critic(state_dim)\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float)\n",
    "value = critic(state)\n",
    "\n",
    "print(f\"estimated state value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 20.0, 100.0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_rewards_to_go(rewards: list[float], gamma: float) -> list[float]:\n",
    "    \n",
    "    rewards = rewards.copy()\n",
    "    rewards_to_go = deque()\n",
    "    \n",
    "    while len(rewards) > 0:\n",
    "        # Iterate the rewards from back to front\n",
    "        reward = rewards.pop()\n",
    "        \n",
    "        # The value of reward-to-go \n",
    "        # associated with the next state\n",
    "        if len(rewards_to_go) > 0:\n",
    "            next_reward_to_go = rewards_to_go[0]\n",
    "        else:\n",
    "            next_reward_to_go = 0\n",
    "        \n",
    "        # Compute reward-go-to associated with current state\n",
    "        reward_to_go = reward + gamma * next_reward_to_go\n",
    "        \n",
    "        # Add to the front of the queue\n",
    "        rewards_to_go.appendleft(reward_to_go)\n",
    "    \n",
    "    # Convert to list\n",
    "    rewards_to_go = list(rewards_to_go)\n",
    "    \n",
    "    return rewards_to_go\n",
    "\n",
    "compute_rewards_to_go([1, 10, 100], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advantages: [-0.98058067 -0.39223227  1.37281295]\n"
     ]
    }
   ],
   "source": [
    "def compute_advantages(\n",
    "        critic: Critic,\n",
    "        rewards_to_go: list[float], \n",
    "        states: list[np.ndarray]\n",
    "    ):\n",
    "    \n",
    "    # State values\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float)\n",
    "    values = critic.evaluate(states)\n",
    "    values = values.detach().numpy()\n",
    "    \n",
    "    # Compute advantages\n",
    "    rewards_to_go = np.array(rewards_to_go)\n",
    "    advantages: np.ndarray = rewards_to_go - values\n",
    "    \n",
    "    # Normalize the advatages\n",
    "    advantages = stats.zscore(advantages)\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "advantages = compute_advantages(\n",
    "    critic,\n",
    "    [1, 2, 5],\n",
    "    [state] * 3\n",
    ")\n",
    "\n",
    "print(f\"advantages: {advantages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\n",
    "        \"state\",\n",
    "        \"action\",\n",
    "        \"reward\",\n",
    "        \"reward_to_go\",\n",
    "        \"log_prob\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def convert_to_transition_with_fields_as_lists(transitions: list[Transition]) -> Transition:\n",
    "    \n",
    "    return Transition(*map(list, zip(*transitions)))\n",
    "    \n",
    "def convert_to_transitions(transition_with_fields_as_list: Transition) -> Transition:\n",
    "    \n",
    "    return list(map(\n",
    "        lambda fields: Transition(*fields), \n",
    "        zip(*transition_with_fields_as_list)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(\n",
    "        env: Env,\n",
    "        actor: Actor,\n",
    "        max_n_timesteps_per_episode: int,\n",
    "        gamma: float = 0.99\n",
    "    ) -> Transition:\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    is_done = False\n",
    "    \n",
    "    for t in range(max_n_timesteps_per_episode):\n",
    "        \n",
    "        # Select an action\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        action = actor.select_action(state)\n",
    "        action = action.detach().numpy()\n",
    "        \n",
    "        # Compute the log-probability of the action taken\n",
    "        log_prob = actor.log_prob()\n",
    "        log_prob = log_prob.detach().item()\n",
    "        \n",
    "        # Interact with the env\n",
    "        next_state, reward, is_terminated, is_truncated, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # The episode ends\n",
    "        is_done = is_terminated or is_truncated\n",
    "        if is_done:\n",
    "            break\n",
    "        \n",
    "        # Step to the next state\n",
    "        state = next_state\n",
    "    \n",
    "    # Compute the rewards-to-go \n",
    "    # based on the received rewards of entire episode\n",
    "    rewards_to_go = compute_rewards_to_go(rewards, gamma)\n",
    "    \n",
    "    return Transition(\n",
    "        state=states,\n",
    "        action=actions,\n",
    "        reward=rewards,\n",
    "        reward_to_go=rewards_to_go,\n",
    "        log_prob=log_probs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(deque, Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            env: Env,\n",
    "            capacity: int,\n",
    "            max_n_timesteps_per_episode: int,\n",
    "            gamma: float\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__(maxlen=capacity)\n",
    "\n",
    "        self._env = env\n",
    "        self._capacity = capacity\n",
    "        self._max_n_timesteps_per_episode = max_n_timesteps_per_episode\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    @property\n",
    "    def capacity(self) -> int:\n",
    "        return self._capacity\n",
    "    \n",
    "    @property\n",
    "    def max_n_timesteps_per_episode(self) -> int:\n",
    "        return self._max_n_timesteps_per_episode\n",
    "        \n",
    "    def collect(self, actor: Actor) -> None:\n",
    "        \n",
    "        while len(self) < self._capacity:\n",
    "            \n",
    "            transition_with_fields_as_list = play_one_episode(\n",
    "                env=self._env,\n",
    "                actor=actor,\n",
    "                max_n_timesteps_per_episode=self._max_n_timesteps_per_episode,\n",
    "                gamma=self._gamma\n",
    "            )\n",
    "            \n",
    "            # Convert to list of transitions\n",
    "            transitions = convert_to_transitions(transition_with_fields_as_list)\n",
    "            \n",
    "            # Add to the buffer\n",
    "            self.extend(transitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_opt = Adam(actor.parameters())\n",
    "critic_opt = Adam(critic.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor_critic(\n",
    "        actor: Actor,\n",
    "        critic: Critic,\n",
    "        actor_opt: Optimizer,\n",
    "        critic_opt: Optimizer,\n",
    "        replay_buffer_loader: DataLoader,\n",
    "        n_epochs: int,\n",
    "        epsilon: float = 0.2\n",
    "    ):\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        transition: Transition\n",
    "        for transition in replay_buffer_loader:\n",
    "            \n",
    "            batch_states = transition.state\n",
    "            batch_actions = transition.action\n",
    "            \n",
    "            log_porbs = actor.log_prob(batch_actions, batch_states)\n",
    "            batch_log_probs = transition.log_prob\n",
    "            \n",
    "            ratios = torch.exp(log_porbs - batch_log_probs)\n",
    "            \n",
    "            batch_rewards_to_go = transition.reward_to_go.type(torch.float)\n",
    "            state_values = critic.evaluate(batch_states)\n",
    "            advantages = batch_rewards_to_go - state_values.detach()\n",
    "        \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clip(\n",
    "                ratios,\n",
    "                1 - epsilon,\n",
    "                1 + epsilon\n",
    "            ) * advantages\n",
    "            \n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Update actor\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "            \n",
    "            \n",
    "            critic_loss = F.mse_loss(batch_rewards_to_go, state_values)\n",
    "\n",
    "            # Update critic\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPOConfig:\n",
    "    \n",
    "    # Total number of epochs of the PPO algorithm\n",
    "    n_epochs: int\n",
    "    \n",
    "    replay_buffer_capacity: int\n",
    "    max_n_timesteps_per_episode: int\n",
    "    \n",
    "    # Discount factor\n",
    "    gamma: float\n",
    "    \n",
    "    # Batch size of the data loader\n",
    "    batch_size: int\n",
    "    \n",
    "    # Number of epochs to update actor and critc networks\n",
    "    n_epochs_for_actor_critic: int\n",
    "    \n",
    "    # Learning rate of the Adam optimizers\n",
    "    lr: float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            env: Env,\n",
    "            config: PPOConfig\n",
    "        ) -> None:\n",
    "        \n",
    "        self._env = env\n",
    "        self._config = config\n",
    "        state_dim = get_state_dim(env)\n",
    "        action_spec = ActionSpec.from_env(env)\n",
    "        \n",
    "        # Actor and critic networks\n",
    "        self._actor = Actor(state_dim, action_spec)\n",
    "        self._critic = Critic(state_dim)\n",
    "        \n",
    "        # Optimizers\n",
    "        self._actor_opt = Adam(self._actor.parameters(), lr=self._config.lr)\n",
    "        self._critic_opt = Adam(self._critic.parameters(), lr=self._config.lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self._replay_buffer = ReplayBuffer(\n",
    "            env=env,\n",
    "            capacity=self._config.replay_buffer_capacity,\n",
    "            max_n_timesteps_per_episode=self._config.max_n_timesteps_per_episode,\n",
    "            gamma=self._config.gamma\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def actor(self) -> Actor:\n",
    "        return self._actor\n",
    "    \n",
    "    @property\n",
    "    def critic(self) -> Critic:\n",
    "        return self._critic\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        for epoch in range(self._config.n_epochs):\n",
    "            \n",
    "            logging.info(f\"PPO epoch: {epoch + 1}\")\n",
    "            avg_episode_rewards = []\n",
    "            \n",
    "            # Collect transitions\n",
    "            self._replay_buffer.collect(self._actor)\n",
    "            \n",
    "            rewards = []\n",
    "            for transition in self._replay_buffer:\n",
    "                rewards.append(transition.reward)\n",
    "            avg_episode_reward = np.mean(rewards)\n",
    "            avg_episode_rewards.append(avg_episode_reward)\n",
    "            logging.info(f\"average episode rewards: {avg_episode_reward}\")\n",
    "            \n",
    "            # Create a data loader\n",
    "            replay_buffer_loader = DataLoader(\n",
    "                self._replay_buffer,\n",
    "                batch_size=self._config.batch_size\n",
    "            )\n",
    "            \n",
    "            # Train the actor and critic   \n",
    "            update_actor_critic(\n",
    "                actor=self._actor,\n",
    "                critic=self._critic,\n",
    "                actor_opt=self._actor_opt,\n",
    "                critic_opt=self._critic_opt,\n",
    "                replay_buffer_loader=replay_buffer_loader,\n",
    "                n_epochs=self._config.n_epochs_for_actor_critic\n",
    "            )\n",
    "            \n",
    "            # Clear replay buffer\n",
    "            self._replay_buffer.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:43:53,491 | INFO | PPO epoch: 1\n",
      "2023-09-21 14:43:54,746 | INFO | average episode rewards: -6.110923935401077\n",
      "2023-09-21 14:43:55,076 | INFO | PPO epoch: 2\n",
      "2023-09-21 14:43:56,339 | INFO | average episode rewards: -6.071260639772841\n",
      "2023-09-21 14:43:56,656 | INFO | PPO epoch: 3\n",
      "2023-09-21 14:43:57,937 | INFO | average episode rewards: -6.286279557600787\n",
      "2023-09-21 14:43:58,241 | INFO | PPO epoch: 4\n",
      "2023-09-21 14:43:59,476 | INFO | average episode rewards: -5.504663757900611\n",
      "2023-09-21 14:43:59,785 | INFO | PPO epoch: 5\n",
      "2023-09-21 14:44:01,035 | INFO | average episode rewards: -6.153235834902657\n",
      "2023-09-21 14:44:01,327 | INFO | PPO epoch: 6\n",
      "2023-09-21 14:44:02,545 | INFO | average episode rewards: -6.730246981997852\n",
      "2023-09-21 14:44:02,837 | INFO | PPO epoch: 7\n",
      "2023-09-21 14:44:04,076 | INFO | average episode rewards: -5.464440702286453\n",
      "2023-09-21 14:44:04,458 | INFO | PPO epoch: 8\n",
      "2023-09-21 14:44:05,671 | INFO | average episode rewards: -5.593644874240346\n",
      "2023-09-21 14:44:05,962 | INFO | PPO epoch: 9\n",
      "2023-09-21 14:44:07,171 | INFO | average episode rewards: -6.399172619649428\n",
      "2023-09-21 14:44:07,462 | INFO | PPO epoch: 10\n",
      "2023-09-21 14:44:08,685 | INFO | average episode rewards: -6.022530136556304\n",
      "2023-09-21 14:44:08,983 | INFO | PPO epoch: 11\n",
      "2023-09-21 14:44:10,215 | INFO | average episode rewards: -6.050024077415373\n",
      "2023-09-21 14:44:10,573 | INFO | PPO epoch: 12\n",
      "2023-09-21 14:44:11,792 | INFO | average episode rewards: -5.595678804214014\n",
      "2023-09-21 14:44:12,085 | INFO | PPO epoch: 13\n",
      "2023-09-21 14:44:13,291 | INFO | average episode rewards: -5.7897880148753895\n",
      "2023-09-21 14:44:13,583 | INFO | PPO epoch: 14\n",
      "2023-09-21 14:44:14,799 | INFO | average episode rewards: -5.937452429329713\n",
      "2023-09-21 14:44:15,093 | INFO | PPO epoch: 15\n",
      "2023-09-21 14:44:16,314 | INFO | average episode rewards: -5.859606695803003\n",
      "2023-09-21 14:44:16,672 | INFO | PPO epoch: 16\n",
      "2023-09-21 14:44:17,887 | INFO | average episode rewards: -5.676640589709954\n",
      "2023-09-21 14:44:18,195 | INFO | PPO epoch: 17\n",
      "2023-09-21 14:44:19,419 | INFO | average episode rewards: -5.387011686028222\n",
      "2023-09-21 14:44:19,744 | INFO | PPO epoch: 18\n",
      "2023-09-21 14:44:20,956 | INFO | average episode rewards: -5.776525081077417\n",
      "2023-09-21 14:44:21,245 | INFO | PPO epoch: 19\n",
      "2023-09-21 14:44:22,450 | INFO | average episode rewards: -5.472712473588778\n",
      "2023-09-21 14:44:22,805 | INFO | PPO epoch: 20\n",
      "2023-09-21 14:44:24,005 | INFO | average episode rewards: -5.2392058947750675\n",
      "2023-09-21 14:44:24,295 | INFO | PPO epoch: 21\n",
      "2023-09-21 14:44:25,506 | INFO | average episode rewards: -6.096322011961469\n",
      "2023-09-21 14:44:25,794 | INFO | PPO epoch: 22\n",
      "2023-09-21 14:44:27,000 | INFO | average episode rewards: -5.723285307642029\n",
      "2023-09-21 14:44:27,288 | INFO | PPO epoch: 23\n",
      "2023-09-21 14:44:28,511 | INFO | average episode rewards: -5.781585879501909\n",
      "2023-09-21 14:44:28,864 | INFO | PPO epoch: 24\n",
      "2023-09-21 14:44:30,132 | INFO | average episode rewards: -5.404026350685467\n",
      "2023-09-21 14:44:30,459 | INFO | PPO epoch: 25\n",
      "2023-09-21 14:44:31,703 | INFO | average episode rewards: -5.59045952709884\n",
      "2023-09-21 14:44:32,024 | INFO | PPO epoch: 26\n",
      "2023-09-21 14:44:33,242 | INFO | average episode rewards: -5.630707363056575\n",
      "2023-09-21 14:44:33,556 | INFO | PPO epoch: 27\n",
      "2023-09-21 14:44:34,826 | INFO | average episode rewards: -5.025507696244329\n",
      "2023-09-21 14:44:35,217 | INFO | PPO epoch: 28\n",
      "2023-09-21 14:44:36,527 | INFO | average episode rewards: -5.972768697684134\n",
      "2023-09-21 14:44:36,915 | INFO | PPO epoch: 29\n",
      "2023-09-21 14:44:38,185 | INFO | average episode rewards: -5.367618255026877\n",
      "2023-09-21 14:44:38,477 | INFO | PPO epoch: 30\n",
      "2023-09-21 14:44:39,777 | INFO | average episode rewards: -5.140174826735197\n",
      "2023-09-21 14:44:40,114 | INFO | PPO epoch: 31\n",
      "2023-09-21 14:44:41,363 | INFO | average episode rewards: -4.981673487006734\n",
      "2023-09-21 14:44:41,729 | INFO | PPO epoch: 32\n",
      "2023-09-21 14:44:42,991 | INFO | average episode rewards: -5.127025520256155\n",
      "2023-09-21 14:44:43,300 | INFO | PPO epoch: 33\n",
      "2023-09-21 14:44:44,525 | INFO | average episode rewards: -5.3419723138993955\n",
      "2023-09-21 14:44:44,813 | INFO | PPO epoch: 34\n",
      "2023-09-21 14:44:46,031 | INFO | average episode rewards: -5.30891222853752\n",
      "2023-09-21 14:44:46,318 | INFO | PPO epoch: 35\n",
      "2023-09-21 14:44:47,529 | INFO | average episode rewards: -5.684426722799187\n",
      "2023-09-21 14:44:47,885 | INFO | PPO epoch: 36\n",
      "2023-09-21 14:44:49,138 | INFO | average episode rewards: -4.8530052231381\n",
      "2023-09-21 14:44:49,477 | INFO | PPO epoch: 37\n",
      "2023-09-21 14:44:50,708 | INFO | average episode rewards: -5.166183728959908\n",
      "2023-09-21 14:44:50,999 | INFO | PPO epoch: 38\n",
      "2023-09-21 14:44:52,218 | INFO | average episode rewards: -4.636682895422886\n",
      "2023-09-21 14:44:52,507 | INFO | PPO epoch: 39\n",
      "2023-09-21 14:44:53,753 | INFO | average episode rewards: -4.71075184322738\n",
      "2023-09-21 14:44:54,224 | INFO | PPO epoch: 40\n",
      "2023-09-21 14:44:55,455 | INFO | average episode rewards: -5.1569811687949425\n",
      "2023-09-21 14:44:55,763 | INFO | PPO epoch: 41\n",
      "2023-09-21 14:44:57,035 | INFO | average episode rewards: -4.79638571747526\n",
      "2023-09-21 14:44:57,336 | INFO | PPO epoch: 42\n",
      "2023-09-21 14:44:58,561 | INFO | average episode rewards: -4.835689811830781\n",
      "2023-09-21 14:44:58,849 | INFO | PPO epoch: 43\n",
      "2023-09-21 14:45:00,107 | INFO | average episode rewards: -4.282140943069902\n",
      "2023-09-21 14:45:00,494 | INFO | PPO epoch: 44\n",
      "2023-09-21 14:45:01,763 | INFO | average episode rewards: -4.510575011972921\n",
      "2023-09-21 14:45:02,050 | INFO | PPO epoch: 45\n",
      "2023-09-21 14:45:03,255 | INFO | average episode rewards: -4.3932612586966036\n",
      "2023-09-21 14:45:03,540 | INFO | PPO epoch: 46\n",
      "2023-09-21 14:45:04,737 | INFO | average episode rewards: -4.474887797060449\n",
      "2023-09-21 14:45:05,021 | INFO | PPO epoch: 47\n",
      "2023-09-21 14:45:06,216 | INFO | average episode rewards: -4.7754210740039325\n",
      "2023-09-21 14:45:06,608 | INFO | PPO epoch: 48\n",
      "2023-09-21 14:45:07,833 | INFO | average episode rewards: -4.3594883025611475\n",
      "2023-09-21 14:45:08,129 | INFO | PPO epoch: 49\n",
      "2023-09-21 14:45:09,351 | INFO | average episode rewards: -4.323283747093484\n",
      "2023-09-21 14:45:09,641 | INFO | PPO epoch: 50\n",
      "2023-09-21 14:45:10,858 | INFO | average episode rewards: -4.232465984200838\n",
      "2023-09-21 14:45:11,172 | INFO | PPO epoch: 51\n",
      "2023-09-21 14:45:12,394 | INFO | average episode rewards: -4.255824298900807\n",
      "2023-09-21 14:45:12,744 | INFO | PPO epoch: 52\n",
      "2023-09-21 14:45:14,001 | INFO | average episode rewards: -4.308886162348113\n",
      "2023-09-21 14:45:14,401 | INFO | PPO epoch: 53\n",
      "2023-09-21 14:45:15,710 | INFO | average episode rewards: -4.323119418666096\n",
      "2023-09-21 14:45:16,027 | INFO | PPO epoch: 54\n",
      "2023-09-21 14:45:17,249 | INFO | average episode rewards: -4.257481836218371\n",
      "2023-09-21 14:45:17,541 | INFO | PPO epoch: 55\n",
      "2023-09-21 14:45:18,771 | INFO | average episode rewards: -4.084458802543108\n",
      "2023-09-21 14:45:19,127 | INFO | PPO epoch: 56\n",
      "2023-09-21 14:45:20,362 | INFO | average episode rewards: -4.169530096280342\n",
      "2023-09-21 14:45:20,654 | INFO | PPO epoch: 57\n",
      "2023-09-21 14:45:21,872 | INFO | average episode rewards: -4.137685485908213\n",
      "2023-09-21 14:45:22,162 | INFO | PPO epoch: 58\n",
      "2023-09-21 14:45:23,372 | INFO | average episode rewards: -4.249553402858064\n",
      "2023-09-21 14:45:23,664 | INFO | PPO epoch: 59\n",
      "2023-09-21 14:45:24,873 | INFO | average episode rewards: -3.964422105153224\n",
      "2023-09-21 14:45:25,227 | INFO | PPO epoch: 60\n",
      "2023-09-21 14:45:26,440 | INFO | average episode rewards: -4.064252007168193\n",
      "2023-09-21 14:45:26,730 | INFO | PPO epoch: 61\n",
      "2023-09-21 14:45:27,941 | INFO | average episode rewards: -3.8129484918542738\n",
      "2023-09-21 14:45:28,243 | INFO | PPO epoch: 62\n",
      "2023-09-21 14:45:29,453 | INFO | average episode rewards: -4.10846290588343\n",
      "2023-09-21 14:45:29,759 | INFO | PPO epoch: 63\n",
      "2023-09-21 14:45:30,966 | INFO | average episode rewards: -4.100469735873232\n",
      "2023-09-21 14:45:31,319 | INFO | PPO epoch: 64\n",
      "2023-09-21 14:45:32,535 | INFO | average episode rewards: -3.988986004465981\n",
      "2023-09-21 14:45:32,823 | INFO | PPO epoch: 65\n",
      "2023-09-21 14:45:34,042 | INFO | average episode rewards: -4.172985700446049\n",
      "2023-09-21 14:45:34,329 | INFO | PPO epoch: 66\n",
      "2023-09-21 14:45:35,544 | INFO | average episode rewards: -3.9492351237683425\n",
      "2023-09-21 14:45:35,834 | INFO | PPO epoch: 67\n",
      "2023-09-21 14:45:37,055 | INFO | average episode rewards: -4.037028037636843\n",
      "2023-09-21 14:45:37,408 | INFO | PPO epoch: 68\n",
      "2023-09-21 14:45:38,632 | INFO | average episode rewards: -3.8717523354590213\n",
      "2023-09-21 14:45:38,919 | INFO | PPO epoch: 69\n",
      "2023-09-21 14:45:40,139 | INFO | average episode rewards: -3.6380695252078232\n",
      "2023-09-21 14:45:40,427 | INFO | PPO epoch: 70\n",
      "2023-09-21 14:45:41,644 | INFO | average episode rewards: -3.7535559594569334\n",
      "2023-09-21 14:45:41,932 | INFO | PPO epoch: 71\n",
      "2023-09-21 14:45:43,145 | INFO | average episode rewards: -3.717212148282896\n",
      "2023-09-21 14:45:43,497 | INFO | PPO epoch: 72\n",
      "2023-09-21 14:45:44,707 | INFO | average episode rewards: -3.8200672704970398\n",
      "2023-09-21 14:45:44,997 | INFO | PPO epoch: 73\n",
      "2023-09-21 14:45:46,217 | INFO | average episode rewards: -3.8056665582783458\n",
      "2023-09-21 14:45:46,505 | INFO | PPO epoch: 74\n",
      "2023-09-21 14:45:47,727 | INFO | average episode rewards: -3.671062882379162\n",
      "2023-09-21 14:45:48,017 | INFO | PPO epoch: 75\n",
      "2023-09-21 14:45:49,240 | INFO | average episode rewards: -3.6105846328232505\n",
      "2023-09-21 14:45:49,601 | INFO | PPO epoch: 76\n",
      "2023-09-21 14:45:50,822 | INFO | average episode rewards: -3.5088284661623703\n",
      "2023-09-21 14:45:51,111 | INFO | PPO epoch: 77\n",
      "2023-09-21 14:45:52,331 | INFO | average episode rewards: -3.463422440033662\n",
      "2023-09-21 14:45:52,619 | INFO | PPO epoch: 78\n",
      "2023-09-21 14:45:53,830 | INFO | average episode rewards: -3.390544654137758\n",
      "2023-09-21 14:45:54,118 | INFO | PPO epoch: 79\n",
      "2023-09-21 14:45:55,336 | INFO | average episode rewards: -3.5680100887619566\n",
      "2023-09-21 14:45:55,688 | INFO | PPO epoch: 80\n",
      "2023-09-21 14:45:56,900 | INFO | average episode rewards: -3.352304634466086\n",
      "2023-09-21 14:45:57,190 | INFO | PPO epoch: 81\n",
      "2023-09-21 14:45:58,413 | INFO | average episode rewards: -3.308571335618333\n",
      "2023-09-21 14:45:58,704 | INFO | PPO epoch: 82\n",
      "2023-09-21 14:45:59,933 | INFO | average episode rewards: -3.363160211521526\n",
      "2023-09-21 14:46:00,222 | INFO | PPO epoch: 83\n",
      "2023-09-21 14:46:01,427 | INFO | average episode rewards: -3.336297793707316\n",
      "2023-09-21 14:46:01,779 | INFO | PPO epoch: 84\n",
      "2023-09-21 14:46:03,000 | INFO | average episode rewards: -3.1232256799079816\n",
      "2023-09-21 14:46:03,288 | INFO | PPO epoch: 85\n",
      "2023-09-21 14:46:04,505 | INFO | average episode rewards: -2.893729036215459\n",
      "2023-09-21 14:46:04,795 | INFO | PPO epoch: 86\n",
      "2023-09-21 14:46:06,015 | INFO | average episode rewards: -2.7299993584978868\n",
      "2023-09-21 14:46:06,303 | INFO | PPO epoch: 87\n",
      "2023-09-21 14:46:07,514 | INFO | average episode rewards: -2.615799076306543\n",
      "2023-09-21 14:46:07,869 | INFO | PPO epoch: 88\n",
      "2023-09-21 14:46:09,112 | INFO | average episode rewards: -2.207611983904887\n",
      "2023-09-21 14:46:09,409 | INFO | PPO epoch: 89\n",
      "2023-09-21 14:46:10,650 | INFO | average episode rewards: -1.9580470846103188\n",
      "2023-09-21 14:46:10,939 | INFO | PPO epoch: 90\n",
      "2023-09-21 14:46:12,155 | INFO | average episode rewards: -1.6758821685748795\n",
      "2023-09-21 14:46:12,446 | INFO | PPO epoch: 91\n",
      "2023-09-21 14:46:13,662 | INFO | average episode rewards: -1.1631944322504268\n",
      "2023-09-21 14:46:14,014 | INFO | PPO epoch: 92\n",
      "2023-09-21 14:46:15,230 | INFO | average episode rewards: -1.1940382901557742\n",
      "2023-09-21 14:46:15,519 | INFO | PPO epoch: 93\n",
      "2023-09-21 14:46:16,730 | INFO | average episode rewards: -1.186452953358785\n",
      "2023-09-21 14:46:17,020 | INFO | PPO epoch: 94\n",
      "2023-09-21 14:46:18,232 | INFO | average episode rewards: -1.1377638725134356\n",
      "2023-09-21 14:46:18,523 | INFO | PPO epoch: 95\n",
      "2023-09-21 14:46:19,759 | INFO | average episode rewards: -1.2804482369598744\n",
      "2023-09-21 14:46:20,116 | INFO | PPO epoch: 96\n",
      "2023-09-21 14:46:21,333 | INFO | average episode rewards: -1.2031686884665314\n",
      "2023-09-21 14:46:21,622 | INFO | PPO epoch: 97\n",
      "2023-09-21 14:46:22,836 | INFO | average episode rewards: -1.217157130940351\n",
      "2023-09-21 14:46:23,124 | INFO | PPO epoch: 98\n",
      "2023-09-21 14:46:24,343 | INFO | average episode rewards: -1.0281718137516214\n",
      "2023-09-21 14:46:24,632 | INFO | PPO epoch: 99\n",
      "2023-09-21 14:46:25,864 | INFO | average episode rewards: -1.0543439617908212\n",
      "2023-09-21 14:46:26,218 | INFO | PPO epoch: 100\n",
      "2023-09-21 14:46:27,457 | INFO | average episode rewards: -0.9634774535826106\n"
     ]
    }
   ],
   "source": [
    "ppo = PPO(\n",
    "    env,\n",
    "    config=PPOConfig(\n",
    "        n_epochs=100,\n",
    "        replay_buffer_capacity=4800,\n",
    "        max_n_timesteps_per_episode=1600,\n",
    "        gamma=0.99,\n",
    "        batch_size=1024,\n",
    "        n_epochs_for_actor_critic=10,\n",
    "        lr=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "ppo.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppo.actor, \"../models/actor.pth\")\n",
    "torch.save(ppo.critic, \"../models/critic.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (mlp): MLP(\n",
      "    (layer1): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (dist_param1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (dist_param2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Critic(\n",
      "  (mlp): MLP(\n",
      "    (layer1): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "actor = torch.load(\"../models/actor.pth\")\n",
    "critic = torch.load(\"../models/critic.pth\")\n",
    "actor.eval()\n",
    "critic.eval()\n",
    "\n",
    "print(actor)\n",
    "print(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/linguaml/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/isaac/Developer/py-projects/linguAML/book/_static/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/isaac/Developer/py-projects/linguAML/book/_static/videos/ppo-Pendulum-v1-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/isaac/Developer/py-projects/linguAML/book/_static/videos/ppo-Pendulum-v1-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/isaac/Developer/py-projects/linguAML/book/_static/videos/ppo-Pendulum-v1-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"Pendulum-v1\", \n",
    "    max_episode_steps=1000,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "env = RecordVideo(\n",
    "    env, \n",
    "    video_folder=\"../_static/videos\",\n",
    "    name_prefix=f\"ppo-{env.spec.id}\"\n",
    ")\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "for t in count():\n",
    "    \n",
    "    # Select an action\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = actor.select_action(state)\n",
    "    action = action.detach().numpy()\n",
    "    \n",
    "    # Interactive with the environment\n",
    "    next_state, reward, is_terminated, is_truncated, info = env.step(action)\n",
    "    \n",
    "    is_done = is_terminated or is_truncated\n",
    "    \n",
    "    if is_done:\n",
    "        break\n",
    "    \n",
    "     # Step to the next state\n",
    "    state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../_static/videos/ppo-Pendulum-v1-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"../_static/videos/ppo-Pendulum-v1-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
